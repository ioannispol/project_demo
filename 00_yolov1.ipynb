{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp yolov1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO V1 implementation\n",
    "\n",
    "> YOLO v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the YOLOv1 will implemented based on the original [paper](https://arxiv.org/pdf/1506.02640v5.pdf)\n",
    "\n",
    "\n",
    "TODO: \n",
    "* [ ] Need to rewrite the plot function so to give the name and the probability prediction for each bounding box\n",
    "* [ ] Get more metrics from the training function (e.g. training and validation losses)\n",
    "* [ ] Write a function that will plot the training and validation loss as well as the training and validation accuracy\n",
    "* [ ] Use the model on the videos that I have from towing tank to see how well the algorithm performs\n",
    "* [ ] Use images/videos with darker light conditions to train and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Yolo works\n",
    "Yolo is an object detection algorithm and uses features that learned from a cnn network to detect objects. When prerforming object detection we want to correctly identify in the image the objects in the given image. Most of the classic aproaches in the object detection algorithms using the sliding window method where the classifier is run over evenly spaces lacations over the entire image. Such types of algorithms are the Deformable Parts Models (DPM), the R-CNN which uses proposal methods to generate the bounding boxes in the given image and then run the classifier on the proposed bounding boxes. This approch, and particullarly the DPM method is slow and not optimal for real time uses, and the improved version of R-CNN models is gaining some speed by strategically selecting interesting regions and run through them the classifier.\n",
    "\n",
    "On the other hand YOLO algorithm based on the idea to split the image in a grid, for axample for a given image we can split it in a 3 by 3 grid (**_SxS = 3x3_**) which gives as 9 cells. As the below image shows, the image consists by a 3 by 3 grid with 9 cells, and each cell has 2 bouning boxes (**_B_**) which finally will give the prediction bounding boxe for the object in the image.\n",
    "\n",
    "<img alt=\"Bounding Boxes\" width=\"500\" caption=\"Bounding Boxes\" src=\"images/image.png\" id=\"bboxs\"/>\n",
    "\n",
    "[//]:![image](../notes/images/image.png)\n",
    "\n",
    "Figure 1\n",
    "\n",
    "Generally, the YOLO algorithm has the following steps:\n",
    "\n",
    "1. Divide the image into cells with an **_SxS_** grid\n",
    "2. Each cell predicts **_B_** bounding boxes (_A cell is responsible for detecting an object if the object's bounding box is within the cell_\n",
    "3. Return bounding boxes above a given confidence threshold. _The algorithm will show only the bounding box with the highest probability confidence (e.g. 0.90) and will reject all boxes with less values than this threshold_.\n",
    "\n",
    "**Note:** In practice will like touse larger values of $S and B$, such as $S = 19$ and $B = 5$ to identify more objects, and each cell will output a prediction with a corresponding bounding box for a given image.\n",
    "\n",
    "The below image shows the YOLO algorithm's result, which returns the bounding boxes for the detected objects. For the algorithm to perform efficiently needs to be trained sufficiently because with each iteration (epoch), the detection accuracy increases. Also, the bounding boxes can be in more than one cells without any issue, and the detection is performed in the cell where the midpoint of the bounding box belongs.\n",
    "\n",
    "<img alt=\"Bounding Boxes2\" width=\"500\" caption=\"Bounding Boxes2\" src=\"images/image2.png\" id=\"bboxs2\"/>\n",
    "\n",
    "[//]: ![image](../notes/images/image2.png)\n",
    "\n",
    "Figure 2\n",
    "\n",
    "The YOLO object detection algorithm is faster architecture because uses one Convolutional Neural Network (CNN) to run all components in the given image in contrast with the naive sliding window approach where for each image the algorithm (DPM, R-CNN etc) needs to scan it step by step to find the region of interest, the detected objects. The R-CNN for example needs classify around 2000 regions per image which makes the algorithm very time consuming and it's not ideal for real time applications.\n",
    "\n",
    "The figure below shows how the YOLO model creates an $S x S$ grid in the input image and then for each grid cell creates multiple bounding boxes as well as class probability map, and at the end gives the final predictions of the objects in the image.\n",
    "\n",
    "<img alt=\"Bounding Boxesy\" width=\"500\" caption=\"Bounding Boxesy\" src=\"images/yolo_paper.png\" id=\"bboxs\"/>\n",
    "\n",
    "[//]: ![image](../notes/images/yolo_paper.png)\n",
    "\n",
    "Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the bouning boxes are encoded in YOLO?\n",
    "One of the most important aspects of this algorithm is the it builds and specifies the bounding boxes, and the other is the the Loss function. The algorithm uses five components to predict an output:\n",
    "\n",
    "1. The centre of a bounding box $(b_x b_y)$ relative to the bounds of the grid cell\n",
    "2. The width $(b_w)$\n",
    "3. The height $(b_h)$. The width and the height of the entire image.\n",
    "4. The class of the object $(c)$\n",
    "5. The prediction confidence $(p_c)$ which is the probability of the existance of an object within the bounding box.\n",
    "\n",
    "Thus, we, optimally, want one bounding box for each object in the given image and we can be sure that only one object will be predicted for each object by taking the midpoint of the cell that is responsible for outputing that object.\n",
    "\n",
    "So, each bounding box for each cell will have $[x_1, y_1, x_2, y_2]$ coordinates where in the YOLO algorithm will be $[x, y, w, h]$\n",
    "\n",
    "* $x$ and $y$ will be the coordinates for object midpoint in cell -> these actually will be between $0 - 1$\n",
    "* $w$ and $h$ will be the width and the heigth of that object relative to the cell -> $w$ can be _greater_ than 1, if the object is wider than the cell, and $h$ can also be _greater_ than 1, if the object is taller than the cell\n",
    "\n",
    "The labels will look like the following:\n",
    "\n",
    "$label_{cell} = [c_1, c_2, ..., c_5, p_c, x, y, w,h]$\n",
    "\n",
    "where:\n",
    "\n",
    "* $c_1$ to $c_5$ will be the dataset classes\n",
    "* $p_c$ probability that there is an object (1 or 0)\n",
    "* $x, y, w,h$ are the coordinates of the bounding boxes\n",
    "\n",
    "\n",
    "Predictions will look very similar, but will output two bouning boxes (will specialise to output different bounfding boxes (wide vs tall).\n",
    "\n",
    "$pred_{cell} = [c_1, c_2, ..., c_5, p_{c_1}, x_1, y_1, w_1, h_1, p_{c_2}, x_2, y_2, w_2, h_2]$\n",
    "\n",
    "**Note:** A cell can only detect one object, this is also one of the YOLO limitations (we can have finer grid to achieve multiple detections as mentioned above.\n",
    "\n",
    "This is for every cell and the **target** shape for one image will be $(S, S, 10)$\n",
    "\n",
    "where:\n",
    "\n",
    "* $S * S$ is the grid size\n",
    "* $5$ is for the class predictions, $1$ is for the probability score, and $4$ is for the bouning boxes\n",
    "\n",
    "The **predictions** shape will be $(S, S, 15)$ where there is and additional probability score and four extra bounding box predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model architecture\n",
    "\n",
    "<img alt=\"YOLO Architecture\" width=\"800\" caption=\"YOLO Architecture\" src=\"images/model.png\" id=\"yolov1\"/>\n",
    "\n",
    "[//]: ![image](../notes/images/model.png)\n",
    "\n",
    "The original YOLO model consists of 24 convolutional layers followed by 2 fully connected layers.\n",
    "The model accepts 448x448 images and at the first layer has a 7x7 kernel with 64 output filters with stride of 2 (**also need to have a padding of 3 to much the dimensions**), also there is a 2x2 Maxpool Layer with the stride of 2. Simillarly, the rest of the model consists of convolutional layers and Maxpool layers except the last two layers where there are a fully conected layers where the first one takes as and input the convolutional output and make it a linear layer of 4096 feature vector and outputs to the fully connected which is reshaped to become a 7 by 7 by 30 which is the final split size of the image ($S = 7$ which is a $7$ x $7$ grid) with a vector output of 30 (in my case this will be 15).\n",
    "\n",
    "To help whith the architecture building it will be usefull to pre-determine the architecure configuration:\n",
    "\n",
    "```python\n",
    "architecture_config = [\n",
    "    # Tuple: (kernel_size, num_filters, stride, padding)\n",
    "    (7, 64, 2, 3), \n",
    "    \"M\",    # M stands for the MaxPolling Layer and has stride 2x2 and kernel 2x2\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1), \n",
    "    (1, 256, 1, 0), \n",
    "    (3, 512, 1, 1), \n",
    "    \"M\",\n",
    "    # List of tuples: (kernel_size, num_filters, stride, padding), num_of_repeats\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0), \n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\", \n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2], \n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1), \n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "The YOLO loss function is the second most important aspect of the algorithm. The basic concept behind all these losses is that are the sum squared error, and if we look at the first part of the loss function is going to be the loss for the box coordinate for the midpoint (taking the $x$ midpoint value and subtractining from the predicted $\\hat{x}$ squared). The $\\mathbb{1}_{ij}^{obj}$ is the identity function which is calculated when there is an object in the cell, so summurizing there is:\n",
    "\n",
    "* $\\mathbb{1}_{i}^{obj}$ is 1 when there is an object in the cell $i$ otherwise is 0.\n",
    "* $\\mathbb{1}_{ij}^{obj}$ is the $j^{th}$ bounding box prediction for the cell $i$ \n",
    "* $\\mathbb{1}_{ij}^{noobj}$ has the same concept with the previous one, except that is 1 when there is no object and 0 when there is an object. \n",
    "\n",
    "So, to know which bounding box is responsible for outputing that bounding box is by looking at the cell and see which of the predicted bounding boxes has the highest Intersection over Union (IoU) value with the target bouning box. The one with the highest IoU will be the responsible bounding box for the prediction and will be send to the loss function. \n",
    "\n",
    "\\begin{align}\n",
    "&\\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(x_i-\\hat{x}_i)^2 + (y_i-\\hat{y}_i)^2 ] \\\\&+ \\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2 +(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2 ]\\\\\n",
    "&+ \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}(C_i - \\hat{C}_i)^2 + \\lambda_{noobj}\\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{noobj}(C_i - \\hat{C}_i)^2 \\\\\n",
    "&+ \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj}\\sum_{c \\in classes}(p_i(c) - \\hat{p}_i(c))^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.random import seed\n",
    "from torch.functional import chain_matmul \n",
    "from torch.nn.modules import padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change to yolo dir: /media/ioannis/DATA/Documents/Machine_learning/Project/src/yolo_v1\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "# Get the correct path for utils.py script\n",
    "if os.getcwd() == '/media/ioannis/DATA/Documents/Machine_learning/Project/src/yolo_v1':\n",
    "    print(f\"The working direcory is: {os.getcwd()}\")\n",
    "else:    \n",
    "    os.chdir(\"/media/ioannis/DATA/Documents/Machine_learning/Project/src/yolo_v1\")\n",
    "    print(f\"Change to yolo dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from utils import intersection_over_union\n",
    "from utils import(\n",
    "    intersection_over_union,\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO model architecure\n",
    "\n",
    "#### Architecture configuration based on YOLO paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3), \n",
    "    \"M\",    # M stands for the MaxPolling Layer and has stride 2x2 and kernel 2x2\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1), \n",
    "    (1, 256, 1, 0), \n",
    "    (3, 512, 1, 1), \n",
    "    \"M\", \n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0), \n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\", \n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2], \n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1), \n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This CNN block is used to as a blueprint of the conv layers for the YoloV1 model.\n",
    "    Need to use convolutional layers multiple times, so we'll use the CNNBlock for easy of use.\n",
    "\n",
    "    Args:\n",
    "        nn ([type]): [description]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "        return x\n",
    "    \n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),  # (S, S, 30) where C + B * 5 = 30\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 735])\n"
     ]
    }
   ],
   "source": [
    "def test(S=7, B=2, C=5):\n",
    "    \"\"\"\n",
    "    A function to test YoloV1 model\n",
    "    \"\"\"\n",
    "    model = YoloV1(split_size=S, num_boxes=B, num_classes=C)\n",
    "    x = torch.randn((2, 3, 448, 448))\n",
    "    print(model(x).shape)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (torch-gpu)",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
